{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# TP4 : Techniques avancées\n",
    "\n",
    "Dans ce TP, vous allez mettre en pratique tout ce que vous avez appris !\n",
    "\n",
    "Ce TP sera **À RENDRE** sur Moodle :\n",
    "- Fichier notebook (`tp4.ipynb`) complété avec vos réponses\n",
    "- Fichier `models_results.csv` détaillant les résultats de vos expérimentations (voir Exercice 1, Q5 et Q6)\n",
    "- Fichier `predictions.csv` contenant les prédictions de votre meilleur modèle sur les données d'utilisation (voir Exercice 1, Q7)\n",
    "\n",
    "Il comptera pour **5 points** dans votre note de TP de l'UE, principalement déterminé en fonction du F2-score de vos prédictions sur les données d'utilisation (dont vous n'aurez pas les labels...).\n",
    "Exemple : vous obtenez un F2-score de 0.68 sur les données d'utilisation, vous obtiendrez $0.68 * 5 = 3.4$ points.\n",
    "\n",
    "Le prof reste souverain et s'autorise à baisser ou invalider la note si votre fichier notebook (`tp4.ipynb`) contient de la triche et/ou ne correspond pas aux résultats que vous fournissez ! Soyez donc honnête :-)\n",
    "\n",
    "Dans la limite de l'utilisation des données **de \"prototypage\"** fournies, vous êtes autorisés à utiliser toutes les techniques vues en cours : encodages, normalisations, équilibrages, suppression ou remplacement des données manquantes, ...\n",
    "\n",
    "Les données dites \"de déploiement\" (ou d'utilisation) doivent être considérées comme inaccessibles : vous ne devrez les utiliser que pour faire vos prédictions finales. Comme si vous les receviez une fois votre modèle de ML entraîné et déployé sur un serveur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce TP, on se place dans la situation de *data scientists* dans un hôpital. Vos collègues médecins essaient de détecter la présence d'une maladie cardiaque à partir de données sur les patients, obtenues par divers examens médicaux.\n",
    "\n",
    "Votre responsable hiérarchique veut que vous entraîniez un modèle de ML qui sera capable de prédire la présence de cette maladie, afin d'accélérer le traitement des patients et d'éviter le recours à des examens complémentaires beaucoup plus coûteux.\n",
    "\n",
    "On veut donc identifier le plus possible de patients malades (= maximiser le rappel !) pour laisser mourir le moins de patients possibles, tout en évitant de prédire les patients sains comme malade (= éviter que la précision ne soit de 0 !), sinon cela prendra trop de temps et coûtera trop cher... Nous utiliserons donc un F2-score comme métrique principale de sélection du meilleur modèle. Les autres métriques seront calculées et mémorisées à titre indicatif."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description des données :\n",
    "\n",
    "| Nom | Description |\n",
    "|-----|-------------|\n",
    "| age | Âge du patient en années |\n",
    "| sex | Sexe du patient (`male` / `female`) |\n",
    "| chest pain | Type de douleur dans la poitrine ressentie |\n",
    "| resting blood pressure (mm Hg) | Pression sanguine au repos |\n",
    "| cholestoral (mg/dl) | Taux de choléstérol dans le sang |\n",
    "| fasting blood sugar > 120 mg/dl | Est-ce que le taux de glucose dans le sang après 8h à jeûn dépasse une valeur normale ? |\n",
    "| resting electrocardiograph | Présence d'une anomalie à l'électrocardiographe (ECG) au repos (`normal` : pas d'anomalie, `abnormal` : anomalie ST-T, `hypertrophy` : probable hypertrophie du ventricule gauche) |\n",
    "| max heart rate | Nombre maximum de battements cardiaques par minutes durant un exercice de résistance à l'effort (ex : vélo) |\n",
    "| exercise induced angina | Est-ce que l'exercice de résistance à l'effort a entraîné une douleur à la poitrine ? |\n",
    "| oldpeak | Dépression du segment ST de l'ECG lors de l'exercice de résistance à l'effort (relativement à l'ECG au repos) |\n",
    "| slope | Forme du segment ST de l'ECG (`upsloping` : hausse, `flat` : plat, `downsloping` : baisse) |\n",
    "| number of colored vessels by fluoroscopy | Le nombre d'artères majeures colorées par fluoroscopie (rayons X ou CT-scan) |\n",
    "| thalassemia | Présence d'une maladie sanguine (`3` : normal, `6` : maladie irréversible, `7` : maladie réversible)\n",
    "| **class** | **Cible de prédiction** : présence (`disease`) ou absence (`no disease`) d'un problème cardiaque |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1 - Prise en main des données et préparation des fichiers de réponses\n",
    "\n",
    "J'attends un certain format dans vos fichiers de réponses (`models_results.csv` et `predictions.csv`) ; cet exercice va vous faire implémenter les fonctions permettant de remplir ces fichiers comme attendu.\n",
    "\n",
    "Comme vu dans le CM4, lorsque l'on prototype et expérimente sur divers algorithmes de ML, il est important de retenir les scores obtenus pour chaque algorithme, chaque préparation de données, et chaque *seed* (graine aléatoire). Cela permettra de sélectionner le meilleur modèle une fois les expérimentations effectuées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Chargez le fichier `data_prototyping.csv` dans un *DataFrame* `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age     sex        chest pain  resting blood pressure (mm Hg)  \\\n",
      "0   44    male      asymptomatic                           120.0   \n",
      "1   36    male  non-anginal pain                           130.0   \n",
      "2   39  female  non-anginal pain                           110.0   \n",
      "3   48    male      asymptomatic                           124.0   \n",
      "4   36    male  non-anginal pain                           150.0   \n",
      "\n",
      "   cholestoral (mg/dl) fasting blood sugar > 120 mg/dl  \\\n",
      "0                169.0                              no   \n",
      "1                209.0                              no   \n",
      "2                182.0                              no   \n",
      "3                274.0                              no   \n",
      "4                160.0                              no   \n",
      "\n",
      "  resting electrocardiograph  max heart rate exercise induced angina  oldpeak  \\\n",
      "0                     normal           144.0                     yes      2.8   \n",
      "1                     normal           178.0                      no      0.0   \n",
      "2                   abnormal           180.0                      no      0.0   \n",
      "3                hypertrophy           166.0                      no      0.5   \n",
      "4                     normal           172.0                      no      0.0   \n",
      "\n",
      "         slope  number of colored vessels by fluoroscopy  thalassemia  \\\n",
      "0  downsloping                                       0.0          6.0   \n",
      "1          NaN                                       NaN          NaN   \n",
      "2          NaN                                       NaN          NaN   \n",
      "3         flat                                       0.0          7.0   \n",
      "4          NaN                                       NaN          NaN   \n",
      "\n",
      "        class  \n",
      "0     disease  \n",
      "1  no disease  \n",
      "2  no disease  \n",
      "3     disease  \n",
      "4  no disease  \n"
     ]
    }
   ],
   "source": [
    "# Charger les fichiers CSV en séparant les données par des points-virgules\n",
    "df = pd.read_csv('data_prototyping.csv', na_values='?')\n",
    "\n",
    "# Afficher un aperçu des données\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Faites une EDA minimale : quelles sont les colonnes, quels sont leurs types de données ? Quelles sont les valeurs uniques pour chaque colonne catégorielle ? Existe-t-il des valeurs manquantes ?\n",
    "\n",
    "Vous ferez une EDA plus poussée dans l'Exercice 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns dans le DataFrame:\n",
      "['age', 'sex', 'chest pain', 'resting blood pressure (mm Hg)', 'cholestoral (mg/dl)', 'fasting blood sugar > 120 mg/dl', 'resting electrocardiograph', 'max heart rate', 'exercise induced angina', 'oldpeak', 'slope', 'number of colored vessels by fluoroscopy', 'thalassemia', 'class']\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns dans le DataFrame:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs uniques pour la colonne sex: ['male' 'female']\n",
      "Valeurs uniques pour la colonne chest pain: ['asymptomatic' 'non-anginal pain' 'atypical angina' 'typical angina']\n",
      "Valeurs uniques pour la colonne fasting blood sugar > 120 mg/dl: ['no' 'yes' nan]\n",
      "Valeurs uniques pour la colonne resting electrocardiograph: ['normal' 'abnormal' 'hypertrophy']\n",
      "Valeurs uniques pour la colonne exercise induced angina: ['yes' 'no' nan]\n",
      "Valeurs uniques pour la colonne slope: ['downsloping' nan 'flat' 'upsloping']\n",
      "Valeurs uniques pour la colonne class: ['disease' 'no disease']\n"
     ]
    }
   ],
   "source": [
    "# Identifier les colonnes catégorielles\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Afficher les valeurs uniques pour chaque colonne catégorielle\n",
    "for col in categorical_columns:\n",
    "    print(f\"Valeurs uniques pour la colonne {col}: {df[col].unique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resting blood pressure (mm Hg)               25\n",
      "cholestoral (mg/dl)                          22\n",
      "fasting blood sugar > 120 mg/dl              23\n",
      "max heart rate                               25\n",
      "exercise induced angina                      25\n",
      "oldpeak                                      27\n",
      "slope                                       198\n",
      "number of colored vessels by fluoroscopy    284\n",
      "thalassemia                                 253\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Vérifier la présence de valeurs manquantes\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Afficher les colonnes avec des valeurs manquantes\n",
    "print(missing_values[missing_values > 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Séparez vos données en `df_train` + `df_test`. Vous êtes libre de choisir la proportion de données dans chaque jeu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions de df_train: (367, 14)\n",
      "Dimensions de df_test: (92, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Séparer les données en jeu d'entraînement et jeu de test (80% entraînement / 20% test)\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, stratify=df['class'], random_state=42)\n",
    "\n",
    "# Vérifier les dimensions des deux DataFrames\n",
    "print(f\"Dimensions de df_train: {df_train.shape}\")\n",
    "print(f\"Dimensions de df_test: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. On veut préparer les données de manière à ce qu'elles soient utilisables par un algorithme de ML. Il existe plusieurs façons de les préparer, comme on l'a vu au TP2 (c'est subjectif !). Chaque façon peut donner des performances différentes ; certains algorithmes fonctionneront mieux avec certaines façons que d'autres...\n",
    "\n",
    "Implémentez la fonction `prepare_data_v1(df)` qui prépare les données d'un *DataFrame* en entrée, de manière la plus simple possible : je recommande un **encodage ordinal** pour commencer et pouvoir tester rapidement vos algorithmes. Supprimez les colonnes contenant trop de NAs (>50%) pour simplifier également, ainsi que les lignes contenant au moins 1 NA.\n",
    "\n",
    "**Attention** : on rappelle que cet encodage fait perdre toute capacité d'interprétation s'il est utilisé sur une variable qui n'a pas de relation d'ordre ! Certains algorithmes auront également de mauvaises performances car ils calculeront une relation statistique qui n'existe pas... Ce n'est pas grave pour cet exercice, cela permet de tester rapidement un algorithme sur nos données.\n",
    "\n",
    "**Attention²** : Votre fonction devra fonctionner sur des sous-ensembles de données ! En effet, en pratique vous ne connaîtrez pas vos données d'utilisation avant d'avoir déployé votre modèle... Votre fonction de préparation devra donc être appliquée sur les données d'utilisations indépendamment des données de prototypage. Faites particulièrement attention à **l'ordre de vos encodages** ! Si votre fonction encode `normal=1, abnormal=2` dans vos données de prototypage, puis `abnormal=1, normal=2` dans vos données d'utilisation, votre modèle de ML prédira n'importe quoi... Je recommande de faire votre encodage à la main, en utilisant par exemple [`df.replace()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html) pour vous assurer que l'encodage sera **cohérent**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_v1(df):\n",
    "    # Remplacer les '?' par NaN\n",
    "    df.replace('?', np.nan, inplace=True)\n",
    "    \n",
    "    # Imputer les valeurs manquantes\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            # Imputation pour les colonnes catégorielles\n",
    "            df[col].fillna(df[col].mode()[0], inplace=True)  # Valeur la plus fréquente\n",
    "        else:\n",
    "            # Imputation pour les colonnes numériques\n",
    "            df[col].fillna(df[col].median(), inplace=True)  # Médiane\n",
    "\n",
    "    # Supprimer les colonnes avec plus de 50% de valeurs manquantes\n",
    "    threshold = len(df) * 0.5\n",
    "    df = df.dropna(axis=1, thresh=threshold)\n",
    "\n",
    "    # Encodage manuel des colonnes catégorielles (sans encoder la cible)\n",
    "    if 'sex' in df.columns:\n",
    "        df['sex'] = df['sex'].replace({'male': 1, 'female': 0})\n",
    "        \n",
    "    if 'chest pain' in df.columns:\n",
    "        df['chest pain'] = df['chest pain'].replace({'typical angina': 1, 'atypical angina': 2, 'non-anginal pain': 3, 'asymptomatic': 4})\n",
    "    \n",
    "    if 'resting electrocardiograph' in df.columns:\n",
    "        df['resting electrocardiograph'] = df['resting electrocardiograph'].replace({'normal': 0, 'abnormal': 1, 'hypertrophy': 2})\n",
    "    \n",
    "    if 'fasting blood sugar > 120 mg/dl' in df.columns:\n",
    "        df['fasting blood sugar > 120 mg/dl'] = df['fasting blood sugar > 120 mg/dl'].replace({'no': 0, 'yes': 1})\n",
    "    \n",
    "    if 'exercise induced angina' in df.columns:\n",
    "        df['exercise induced angina'] = df['exercise induced angina'].replace({'no': 0, 'yes': 1})\n",
    "    \n",
    "    if 'slope' in df.columns:\n",
    "        df['slope'] = df['slope'].replace({'upsloping': 1, 'flat': 2, 'downsloping': 3})\n",
    "    \n",
    "    if 'thalassemia' in df.columns:\n",
    "        df['thalassemia'] = df['thalassemia'].replace({'3': 0, '6': 1, '7': 2})\n",
    "\n",
    "    # Conversion des colonnes numériques\n",
    "    for col in ['resting blood pressure (mm Hg)', 'cholestoral (mg/dl)', 'max heart rate', 'oldpeak', 'number of colored vessels by fluoroscopy']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(float)\n",
    "\n",
    "    # S'assurer que 'thalassemia' est toujours présente\n",
    "    if 'thalassemia' not in df.columns:\n",
    "        df['thalassemia'] = 3  # Valeur par défaut\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous avez bien séparés vos données et implémenté la fonction de préparation, la prochaine cellule devrait s'exécuter sans problème. Notez bien que les données de test ne sont pas préparées en même temps que les données d'entraînement !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)  # Médiane\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mode()[0], inplace=True)  # Valeur la plus fréquente\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)  # Médiane\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)  # Médiane\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mode()[0], inplace=True)  # Valeur la plus fréquente\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)  # Médiane\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mode()[0], inplace=True)  # Valeur la plus fréquente\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)  # Médiane\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mode()[0], inplace=True)  # Valeur la plus fréquente\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)  # Médiane\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)  # Médiane\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mode()[0], inplace=True)  # Valeur la plus fréquente\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:20: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['sex'] = df['sex'].replace({'male': 1, 'female': 0})\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:23: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['chest pain'] = df['chest pain'].replace({'typical angina': 1, 'atypical angina': 2, 'non-anginal pain': 3, 'asymptomatic': 4})\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:26: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['resting electrocardiograph'] = df['resting electrocardiograph'].replace({'normal': 0, 'abnormal': 1, 'hypertrophy': 2})\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:29: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['fasting blood sugar > 120 mg/dl'] = df['fasting blood sugar > 120 mg/dl'].replace({'no': 0, 'yes': 1})\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:32: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['exercise induced angina'] = df['exercise induced angina'].replace({'no': 0, 'yes': 1})\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:35: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['slope'] = df['slope'].replace({'upsloping': 1, 'flat': 2, 'downsloping': 3})\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)  # Médiane\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mode()[0], inplace=True)  # Valeur la plus fréquente\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)  # Médiane\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)  # Médiane\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mode()[0], inplace=True)  # Valeur la plus fréquente\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)  # Médiane\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mode()[0], inplace=True)  # Valeur la plus fréquente\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)  # Médiane\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mode()[0], inplace=True)  # Valeur la plus fréquente\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)  # Médiane\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)  # Médiane\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mode()[0], inplace=True)  # Valeur la plus fréquente\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:20: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['sex'] = df['sex'].replace({'male': 1, 'female': 0})\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:23: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['chest pain'] = df['chest pain'].replace({'typical angina': 1, 'atypical angina': 2, 'non-anginal pain': 3, 'asymptomatic': 4})\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:26: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['resting electrocardiograph'] = df['resting electrocardiograph'].replace({'normal': 0, 'abnormal': 1, 'hypertrophy': 2})\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:29: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['fasting blood sugar > 120 mg/dl'] = df['fasting blood sugar > 120 mg/dl'].replace({'no': 0, 'yes': 1})\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:32: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['exercise induced angina'] = df['exercise induced angina'].replace({'no': 0, 'yes': 1})\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:35: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['slope'] = df['slope'].replace({'upsloping': 1, 'flat': 2, 'downsloping': 3})\n"
     ]
    }
   ],
   "source": [
    "df_train_v1, df_test_v1 = prepare_data_v1(df_train), prepare_data_v1(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     age  sex  chest pain  resting blood pressure (mm Hg)  \\\n",
      "109   51    0           3                           150.0   \n",
      "270   55    1           3                           110.0   \n",
      "194   49    1           4                           140.0   \n",
      "357   32    1           2                           125.0   \n",
      "164   58    1           3                           105.0   \n",
      "\n",
      "     cholestoral (mg/dl)  fasting blood sugar > 120 mg/dl  \\\n",
      "109                200.0                                0   \n",
      "270                277.0                                0   \n",
      "194                228.0                                0   \n",
      "357                254.0                                0   \n",
      "164                240.0                                0   \n",
      "\n",
      "     resting electrocardiograph  max heart rate  exercise induced angina  \\\n",
      "109                           0           120.0                        0   \n",
      "270                           0           160.0                        0   \n",
      "194                           0           130.0                        0   \n",
      "357                           0           155.0                        0   \n",
      "164                           2           154.0                        1   \n",
      "\n",
      "     oldpeak  slope  number of colored vessels by fluoroscopy  thalassemia  \\\n",
      "109      0.5      1                                       0.0          3.0   \n",
      "270      0.0      2                                       0.0          3.0   \n",
      "194      0.0      2                                       0.0          3.0   \n",
      "357      0.0      2                                       0.0          3.0   \n",
      "164      0.6      2                                       0.0          7.0   \n",
      "\n",
      "          class  \n",
      "109  no disease  \n",
      "270  no disease  \n",
      "194  no disease  \n",
      "357  no disease  \n",
      "164  no disease  \n",
      "     age  sex  chest pain  resting blood pressure (mm Hg)  \\\n",
      "14    39    0           3                            94.0   \n",
      "425   56    1           4                           150.0   \n",
      "444   35    0           4                           140.0   \n",
      "265   66    1           4                           120.0   \n",
      "249   58    1           4                           132.0   \n",
      "\n",
      "     cholestoral (mg/dl)  fasting blood sugar > 120 mg/dl  \\\n",
      "14                 199.0                                0   \n",
      "425                230.0                                0   \n",
      "444                167.0                                0   \n",
      "265                302.0                                0   \n",
      "249                458.0                                1   \n",
      "\n",
      "     resting electrocardiograph  max heart rate  exercise induced angina  \\\n",
      "14                            0           179.0                        0   \n",
      "425                           1           124.0                        1   \n",
      "444                           0           150.0                        0   \n",
      "265                           2           151.0                        0   \n",
      "249                           0            69.0                        0   \n",
      "\n",
      "     oldpeak  slope  number of colored vessels by fluoroscopy  thalassemia  \\\n",
      "14       0.0      1                                       0.0          3.0   \n",
      "425      1.5      2                                       0.0          3.0   \n",
      "444      0.0      1                                       0.0          3.0   \n",
      "265      0.4      2                                       0.0          3.0   \n",
      "249      1.0      3                                       0.0          3.0   \n",
      "\n",
      "          class  \n",
      "14   no disease  \n",
      "425     disease  \n",
      "444  no disease  \n",
      "265  no disease  \n",
      "249  no disease  \n"
     ]
    }
   ],
   "source": [
    "# Afficher un aperçu des données préparées\n",
    "print(df_train_v1.head())\n",
    "print(df_test_v1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Implémentez la méthode `evaluate_model(df_train, df_test, data_name, model_name, model_class, hyperparameters)`. Cette méthode devra instancier un estimateur (`model_class`) avec les hyperparamètres demandés, l'entraîner sur les données d'entraînement fournies, puis calculer les métriques suivantes sur les données de test fournies :\n",
    "\n",
    "- F2-score\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "\n",
    "Vous pouvez utiliser les méthodes de Scikit-learn pour ces [métriques](https://scikit-learn.org/stable/api/sklearn.metrics.html) (attention à la classe d'intérêt ! Par défaut, Scikit utilise `1` comme classe d'intérêt, mais vous pouvez en spécifier une autre via l'argument `pos_label`).\n",
    "\n",
    "Votre fonction devra renvoyer un *DataFrame* contenant les colonnes suivantes :\n",
    "- ModelName : le nom du modèle utilisé (algorithme)\n",
    "- Data : le nom de la méthode de préparation des données\n",
    "- Hyperparamètres : le dictionnaire des hyperparamètres\n",
    "- RandomSeed : la graine aléatoire utilisée pour instancier et entraîner le modèle\n",
    "- Score_f2 : la métrique de F2-Score sur les prédictions de test\n",
    "- Score_precision : la métrique de Precision sur les prédictions de test\n",
    "- Score_recall : la métrique de Rappel sur les prédictions de test\n",
    "- Score_accuracy : la métrique d'Accuracy (% de prédictions correctes) sur les prédictions de test\n",
    "- TrainedEstimator : l'instance du modèle entraînée (pour réutilisation ultérieure si besoin)\n",
    "\n",
    "\n",
    "On vous donne le squelette de cette fonction, à vous de remplir les trous !\n",
    "\n",
    "```python\n",
    "def evaluate_model(df_train, df_test, data_name, model_name, model_class, hyperparameters):\n",
    "    X_train = # À COMPLÉTER\n",
    "    Y_train = # À COMPLÉTER\n",
    "    X_test = # À COMPLÉTER\n",
    "    Y_test = # À COMPLÉTER\n",
    "\n",
    "    # On génère une graine aléatoire ... de manière aléatoire :-)\n",
    "    seed = np.random.randint(0, 2**32 - 1)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # On instancie l'algorithme avec les hyperparamètres demandés\n",
    "    # En pratique, si on a par exemple `model_class=LogisticRegression` et\n",
    "    # # `hyperparameters={ 'n_iter': 100, 'eta0': 0.1 }`, c'est équivalent à faire\n",
    "    # `LogisticRegression(n_iter=100, eta0=0.1)`\n",
    "    estimator = model_class(**hyperparameters)\n",
    "\n",
    "    # À COMPLÉTER : entraînez le modèle !\n",
    "    \n",
    "    y_pred = # À COMPLÉTER : calculez les prédictions sur les données de test\n",
    "\n",
    "    return pd.DataFrame([{\n",
    "        'ModelName': model_name,\n",
    "        'Data': data_name,\n",
    "        'Hyperparameters': hyperparameters,\n",
    "        'RandomSeed': seed,\n",
    "        'Score_f2': # À COMPLÉTER\n",
    "        'Score_precision': # À COMPLÉTER\n",
    "        'Score_recall': # À COMPLÉTER\n",
    "        'Score_accuracy': # À COMPLÉTER\n",
    "        'TrainedEstimator': estimator\n",
    "    }])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "def evaluate_model(df_train, df_test, data_name, model_name, model_class, hyperparameters):\n",
    "    # Vérifier si la colonne 'class' est présente dans df_train et df_test\n",
    "    if 'class' not in df_train.columns or 'class' not in df_test.columns:\n",
    "        raise KeyError(\"'class' column not found in df_train or df_test\")\n",
    "    \n",
    "    # Séparation des features (X) et de la cible (Y)\n",
    "    X_train = df_train.drop(columns=['class'])\n",
    "    Y_train = df_train['class']\n",
    "    X_test = df_test.drop(columns=['class'])\n",
    "    Y_test = df_test['class']\n",
    "    \n",
    "    # Générer une graine aléatoire\n",
    "    seed = np.random.randint(0, 2**32 - 1)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Instancier le modèle avec les hyperparamètres\n",
    "    estimator = model_class(**hyperparameters)\n",
    "    \n",
    "    # Entraîner le modèle\n",
    "    estimator.fit(X_train, Y_train)\n",
    "    \n",
    "    # Prédictions sur les données de test\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    \n",
    "    # Calcul des métriques avec pos_label='disease'\n",
    "    score_f2 = fbeta_score(Y_test, y_pred, beta=2, pos_label='disease')\n",
    "    score_precision = precision_score(Y_test, y_pred, pos_label='disease')\n",
    "    score_recall = recall_score(Y_test, y_pred, pos_label='disease')\n",
    "    score_accuracy = accuracy_score(Y_test, y_pred)\n",
    "    \n",
    "    # Retourner les résultats\n",
    "    return pd.DataFrame([{\n",
    "        'ModelName': model_name,\n",
    "        'Data': data_name,\n",
    "        'Hyperparameters': hyperparameters,\n",
    "        'RandomSeed': seed,\n",
    "        'Score_f2': score_f2,\n",
    "        'Score_precision': score_precision,\n",
    "        'Score_recall': score_recall,\n",
    "        'Score_accuracy': score_accuracy,\n",
    "        'TrainedEstimator': estimator\n",
    "    }])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous avez bien implémenté la fonction `evaluate_model()` et que vos données sont bien préparées, l'expérimentation ci-dessous devrait fonctionner et vous afficher le résultat d'une Régression Logistique (sur des données préparées \"naïvement\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ModelName</th>\n",
       "      <th>Data</th>\n",
       "      <th>Hyperparameters</th>\n",
       "      <th>RandomSeed</th>\n",
       "      <th>Score_f2</th>\n",
       "      <th>Score_precision</th>\n",
       "      <th>Score_recall</th>\n",
       "      <th>Score_accuracy</th>\n",
       "      <th>TrainedEstimator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>v1</td>\n",
       "      <td>{'penalty': None, 'max_iter': 1000}</td>\n",
       "      <td>657861340</td>\n",
       "      <td>0.45977</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.815217</td>\n",
       "      <td>LogisticRegression(max_iter=1000, penalty=None)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ModelName Data                      Hyperparameters  RandomSeed  \\\n",
       "0  LogisticRegression   v1  {'penalty': None, 'max_iter': 1000}   657861340   \n",
       "\n",
       "   Score_f2  Score_precision  Score_recall  Score_accuracy  \\\n",
       "0   0.45977         0.533333      0.444444        0.815217   \n",
       "\n",
       "                                  TrainedEstimator  \n",
       "0  LogisticRegression(max_iter=1000, penalty=None)  "
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Entraîner et évaluer le modèle\n",
    "results = evaluate_model(\n",
    "    df_train_v1,\n",
    "    df_test_v1,\n",
    "    'v1',\n",
    "    'LogisticRegression',\n",
    "    LogisticRegression,\n",
    "    {'penalty': None, 'max_iter': 1000}\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Enregistrez les résultats de l'expérimentation ci-dessus dans un fichier `models_results.csv`.\n",
    "\n",
    "On vous donne, pour vous aider, une fonction qui ajoute automatiquement les résultats sans effacer les précédents ! :-)\n",
    "\n",
    "Note : il est très important d'enregistrer vos résultats à chaque expérimentation, pour ne pas \"perdre\" ces résultats, en particulier si vous redémarrez votre notebook. Dans la suite du TP, utilisez cette fonction **chaque fois** que vous faites une nouvelle expérimentation ! En pratique, on utilise souvent des outils dédiés tels que ceux présentés en cours (MLFlow, Weights and Biases, Aimstack, ...).\n",
    "\n",
    "Le fichier `models_results.csv` sera à rendre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_results(results):\n",
    "    with open('models_results.csv', 'a') as f:\n",
    "        # Quand le fichier est vide, il faut préciser les colonnes ;\n",
    "        # si on a au moins une ligne, il ne faut pas répéter les noms des colonnes...\n",
    "        header = f.seek(0, 2) == 0\n",
    "        # On ne veut pas écrire la colonne `TrainedEstimator` (c'est un objet Python !)\n",
    "        columns = [\n",
    "            'ModelName', 'Data', 'Hyperparameters', 'RandomSeed', 'Score_f2', \n",
    "            'Score_precision', 'Score_recall', 'Score_accuracy',\n",
    "        ]\n",
    "        results.to_csv(f, header=header, columns=columns, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les résultats ont été enregistrés dans 'models_results.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Appeler la fonction pour ajouter les résultats dans le fichier CSV\n",
    "append_results(results)\n",
    "\n",
    "# Vérification : Afficher les résultats enregistrés\n",
    "print(\"Les résultats ont été enregistrés dans 'models_results.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Pour vous montrer ce qu'on attend pour les prédictions à rendre, faites les prédictions sur le modèle de Régression Logistique de la précédente expérimentation, sur les données \"de déploiement\", préparées via la façon 'v1'.\n",
    "\n",
    "On vous fournit une fonction pour écrire les prédictions dans un fichier `predictions.csv`.\n",
    "\n",
    "**Attention** : dans la suite du TP, vous ne devrez faire les prédictions sur ces données de déploiement que pour votre meilleur modèle et votre meilleure préparation de données ! De toute façon vous n'aurez pas les labels, donc vous ne saurez pas si vos prédictions sont bonnes ou non...\n",
    "\n",
    "1. Récupérez le modèle entraîné (`TrainedEstimator`) de votre meilleure expérimentation (pour l'instant vous n'en avez qu'une seule : `results`). Si vous redémarrez le notebook d'ici la fin du TP, vous devriez avoir toutes les informations dans le fichier `models_results.csv` (la classe à utiliser, la fonction de préparation de données, les hyperparamètres, et la *seed* aléatoire) pour ré-entraîner le même modèle.\n",
    "2. Puis, appelez la fonction `write_predictions()` avec en paramètre votre fonction de préparation de données (ici, `prepare_data_v1`) et votre modèle entraîné.\n",
    "\n",
    "Cette fonction se charge automatiquement de lire les données, les préparer, faire les prédictions, et les écrire dans le fichier de sortie `predictions.csv`. Elle gère également les prédictions manquantes si votre fonction de préparations de données supprime des lignes (NAs).\n",
    "\n",
    "Le fichier `predictions.csv` sera à rendre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions(prepare_data_function, estimator):\n",
    "    df_deployment = pd.read_csv('data_deployment.csv')\n",
    "    df_deployment_prepared = prepare_data_function(df_deployment)\n",
    "    # /!\\ Si la fonction de préparation des données supprime des lignes\n",
    "    # (par exemple à cause de la gestion des NAs), on va se retrouver \n",
    "    # avec moins de prédictions qu'on a de lignes...\n",
    "    # On doit matcher chaque prédiction avec sa ligne correspondante\n",
    "    handled_lines = df_deployment_prepared.index\n",
    "\n",
    "    # On prédit pour chaque ligne des données préparées\n",
    "    predictions = estimator.predict(df_deployment_prepared)\n",
    "    # On ré-assimile aux numéros de lignes des données originales\n",
    "    predictions = pd.Series(predictions, index=handled_lines)\n",
    "\n",
    "    # On ajoute la colonne aux données originales ; puisqu'on a indexé les\n",
    "    # prédictions, les prédictions manquantes seront automatiquement remplacées par NaN\n",
    "    df_deployment['predictions'] = predictions\n",
    "    # On écrit le fichier\n",
    "    df_deployment.to_csv('predictions.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les prédictions ont été enregistrées dans 'predictions.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)  # Médiane\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mode()[0], inplace=True)  # Valeur la plus fréquente\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:20: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['sex'] = df['sex'].replace({'male': 1, 'female': 0})\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:23: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['chest pain'] = df['chest pain'].replace({'typical angina': 1, 'atypical angina': 2, 'non-anginal pain': 3, 'asymptomatic': 4})\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:26: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['resting electrocardiograph'] = df['resting electrocardiograph'].replace({'normal': 0, 'abnormal': 1, 'hypertrophy': 2})\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:29: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['fasting blood sugar > 120 mg/dl'] = df['fasting blood sugar > 120 mg/dl'].replace({'no': 0, 'yes': 1})\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:32: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['exercise induced angina'] = df['exercise induced angina'].replace({'no': 0, 'yes': 1})\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:35: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['slope'] = df['slope'].replace({'upsloping': 1, 'flat': 2, 'downsloping': 3})\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3761464116.py:38: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['thalassemia'] = df['thalassemia'].replace({'3': 0, '6': 1, '7': 2})\n"
     ]
    }
   ],
   "source": [
    "trained_estimator = results['TrainedEstimator'].iloc[0]  # Récupérer le modèle entraîné\n",
    "\n",
    "write_predictions(prepare_data_v1, trained_estimator)    # Utiliser prepare_data_v1 pour préparer les données de prédiction\n",
    "\n",
    "# Vérification\n",
    "print(\"Les prédictions ont été enregistrées dans 'predictions.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2 : à vous de jouer !\n",
    "\n",
    "Vous êtes maintenant libres de vos choix pour identifier le meilleur modèle et la meilleure préparation de données !\n",
    "\n",
    "N'oubliez pas d'enregistrer chaque expérimentation via la méthode `append_results()`.\n",
    "\n",
    "**Attention** : une fois que vous avez expérimenté avec une façon de préparer de vos données (par exemple, `prepare_data_v1()`), vous NE DEVEZ PLUS modifier cette fonction ! Sinon, vous rendez irreproductibles toutes vos expérimentations précédentes ! Dans la pratique, on utilise souvent des Data Version Control (DVC) et des Version Control System (VCS, exemple : Git) pour s'assurer de mémoriser ce qu'on a fait et de pouvoir y revenir.\n",
    "\n",
    "Vous devrez donc implémenter des fonctions `prepare_data_v2()`, `prepare_data_v3()`, etc. (autant que nécessaires pour tester toutes vos idées) !\n",
    "\n",
    "On vous donne quelques pistes d'exploration ci-dessous pour vous aider ...\n",
    "Les questions suivantes sont données à titre d'indication, vous n'êtes pas obligés de toutes les faire, ni de suivre cet ordre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Faites une EDA plus poussée. Quelles sont les données catégorielles ? Quelles valeurs peuvent-elles prendre ? Existe-t-il des données manquantes ?\n",
    "\n",
    "Astuce : les types de données que Pandas vous retourne peuvent ne pas être cohérents ! Vérifiez avec la description des données fournie pour détecter si des colonnes devraient plutôt avoir un autre type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. L'encodage ordinal est un moyen rapide de tester un algorithme (prototypage), mais qui montre vite ses limites, surtout pour les données sans relation d'ordre. Quels autres encodages pouvez-vous utiliser ?\n",
    "\n",
    "Astuce : il vaut peut-être le coup d'essayer plusieurs formes d'encodages pour voir l'impact sur les performances...\n",
    "\n",
    "On rappelle quelques méthodes d'encodage :\n",
    "\n",
    "- Encodage ordinal\n",
    "- Encodage one-hot\n",
    "- Encodage binaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Quelles sont les échelles de chaque colonne numérique (valeurs min vs max) ? Quelles sont les distributions de ces colonnes ? Quelles normalisations pouvez-vous utiliser ?\n",
    "\n",
    "Astuce : il vaut peut-être le coup d'essayer plusieurs formes de normalisations pour voir l'impact sur les performances...\n",
    "\n",
    "On rappelle quelques méthodes de normalisation :\n",
    "- Linéaire (min-max)\n",
    "- Z-score\n",
    "- Log\n",
    "\n",
    "Vous pouvez utiliser du *clipping* en plus de chacune des méthodes de normalisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Existe-t-il des données manquantes ? Quelles colonnes ont beaucoup de données manquantes ? Combien de lignes possèdent au moins une valeur manquante ?\n",
    "\n",
    "**Attention** : si votre fonction de préparation de données supprime les lignes contenant 1 valeur manquante (NA), vous allez supprimer des lignes du jeu de données \"de déploiement\" (ou d'utilisation). Ce qui veut dire que vous ne ferez pas de prédictions sur ces lignes, ce qui baissera forcément votre score ! Durant la phase d'entraînement, on peut se permettre de supprimer les valeurs manquantes si on pense qu'elles vont empêcher l'apprentissage ; mais, en déploiement, quand votre modèle sera utilisé par des personnes extérieures, vous pouvez difficilement vous permettre de rejeter des entrées... (Ou alors, vous risquez que vos utilisateurs finissent par ne plus utiliser votre modèle)\n",
    "\n",
    "Vous pouvez essayer de supprimer les données manquantes, ou de les remplacer...\n",
    "\n",
    "On rappelle quelques méthodes de remplacement :\n",
    "- Par un valeur \"sentinelle\" indiquant une absence (par exemple, `-1`)\n",
    "- Par moyenne, médiane, ou mode (valeur fixe)\n",
    "- Par *forward-fill* ou *backward-fill*\n",
    "- Par apprentissage non-supervisé\n",
    "\n",
    "Les méthodes par valeur fixe (moyenne, médiane, mode) ou par données existantes (forward-fill, backward-fill) peuvent être utilisées en corrélation avec d'autres colonnes, par exemple en faisant un `groupby`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. On a essayé une Régression Logistique, mais il existe d'autres algorithmes de classification ! Lesquels paraissent intéressants pour cette tâche ?\n",
    "Comparez ces algorithmes, sur vos différentes façons de préparer vos données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Chaque algorithme possède un certain nombre d'hyperparamètres, impactant les performances. Comparez les performances de différents hyperparamètres, pour chaque modèle.\n",
    "\n",
    "La documentation de Scikit-learn liste les hyperparamètres dans le constructeur de chaque algorithme. Attention : cette liste est souvent plus étoffée que celle vue en cours, car Scikit propose des optimisations, mais laisse la configuration de ces optimisations au choix des utilisateurs..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Quelle est la fréquence de chaque classe ? Vous pouvez essayer de ré-équilibrer les données pour améliorer les performances.\n",
    "\n",
    "On rappelle quelques méthodes de ré-équilibrage :\n",
    "- *Downsampling* : diminution de la classe majoritaire jusqu'à atteindre la même proportion que la classe minoritaire.\n",
    "- *Upsampling* : répétition des individus de la classe minoritaire jusqu'à atteindre la même proportion que la classe majoritaire.\n",
    "- *Loss weighting* : on peut pondérer la fonction de *loss* pour rendre les erreurs sur la classe minoritaire plus coûteuses. Attention : cela suppose que la distribution des classes dans les données d'utilisation seront les mêmes ! Dans la pratique, vous ne pouvez pas en être certains...\n",
    "- Génération de données : on peut générer des données similaires aux données existantes, en espérent qu'elles soient suffisamment proches pour qu'elles représentent bien le phénomène que l'on cherche à modéliser. Attention : dans la pratique, c'est compliqué de générer des données pour modéliser un phénomène, sans avoir un modèle du phénomène ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Dans la fonction `evaluate_model()`, on vous fait initialement mesurer les performances de votre modèle sur les données de test. Or, en comparant les modèles sur ces données, on risque de choisir un modèle qui est trop spécialisé sur ces données, mais qui ne saura pas généraliser aux données d'utilisation ! On a vu en cours une technique pour éviter ce problème : la *cross-validation*. Vous pouvez implémenter une autre fonction `evaluate_model_cv()` qui suit le même principe mais qui utilise la *cross-validation* pour comparer les modèles.\n",
    "\n",
    "Astuce : votre fonction peut évaluer plusieurs modèles et retourner un *DataFrame* contenant plusieurs résultats, avec `pd.DataFrame([ {...}, {...}, {...} ])`, où chaque `{...}` est un dictionnaire comme celui de la fonction `evalute_model()`. Cela permettrait d'évaluer les modèles sur les mêmes *folds* de la *cross-validation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. Vous pouvez essayer des méthodes ensemblistes.\n",
    "\n",
    "On rappelle les 3 principales catégories de méthodes ensemblistes vues en cours :\n",
    "- *Bagging*\n",
    "- *Boosting*\n",
    "- *Stacking*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                                           int64\n",
      "sex                                          object\n",
      "chest pain                                   object\n",
      "resting blood pressure (mm Hg)              float64\n",
      "cholestoral (mg/dl)                         float64\n",
      "fasting blood sugar > 120 mg/dl              object\n",
      "resting electrocardiograph                   object\n",
      "max heart rate                              float64\n",
      "exercise induced angina                      object\n",
      "oldpeak                                     float64\n",
      "slope                                        object\n",
      "number of colored vessels by fluoroscopy    float64\n",
      "thalassemia                                 float64\n",
      "class                                        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Charger les données de prototypage\n",
    "df = pd.read_csv('data_prototyping.csv', na_values='?')\n",
    "\n",
    "# Vérifier les types de données\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes catégorielles : ['sex', 'chest pain', 'fasting blood sugar > 120 mg/dl', 'resting electrocardiograph', 'exercise induced angina', 'slope', 'class']\n",
      "Colonnes numériques : ['age', 'resting blood pressure (mm Hg)', 'cholestoral (mg/dl)', 'max heart rate', 'oldpeak', 'number of colored vessels by fluoroscopy', 'thalassemia']\n"
     ]
    }
   ],
   "source": [
    "# Identifier les colonnes numériques et catégorielles\n",
    "categorical_columns = []\n",
    "numerical_columns = []\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        categorical_columns.append(col)\n",
    "    else:\n",
    "        numerical_columns.append(col)\n",
    "\n",
    "print(\"Colonnes catégorielles :\", categorical_columns)\n",
    "print(\"Colonnes numériques :\", numerical_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valeurs uniques pour la colonne 'sex':\n",
      "['male' 'female']\n",
      "\n",
      "Valeurs uniques pour la colonne 'chest pain':\n",
      "['asymptomatic' 'non-anginal pain' 'atypical angina' 'typical angina']\n",
      "\n",
      "Valeurs uniques pour la colonne 'fasting blood sugar > 120 mg/dl':\n",
      "['no' 'yes' nan]\n",
      "\n",
      "Valeurs uniques pour la colonne 'resting electrocardiograph':\n",
      "['normal' 'abnormal' 'hypertrophy']\n",
      "\n",
      "Valeurs uniques pour la colonne 'exercise induced angina':\n",
      "['yes' 'no' nan]\n",
      "\n",
      "Valeurs uniques pour la colonne 'slope':\n",
      "['downsloping' nan 'flat' 'upsloping']\n",
      "\n",
      "Valeurs uniques pour la colonne 'class':\n",
      "['disease' 'no disease']\n"
     ]
    }
   ],
   "source": [
    "for col in categorical_columns:\n",
    "    print(f\"\\nValeurs uniques pour la colonne '{col}':\")\n",
    "    print(df[col].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valeurs manquantes par colonne :\n",
      "age                                           0\n",
      "sex                                           0\n",
      "chest pain                                    0\n",
      "resting blood pressure (mm Hg)               25\n",
      "cholestoral (mg/dl)                          22\n",
      "fasting blood sugar > 120 mg/dl              23\n",
      "resting electrocardiograph                    0\n",
      "max heart rate                               25\n",
      "exercise induced angina                      25\n",
      "oldpeak                                      27\n",
      "slope                                       198\n",
      "number of colored vessels by fluoroscopy    284\n",
      "thalassemia                                 253\n",
      "class                                         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "print(\"\\nValeurs manquantes par colonne :\")\n",
    "print(missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_v2(df):\n",
    "    # Importer les bibliothèques nécessaires\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # Copier le DataFrame pour éviter de le modifier directement\n",
    "    df = df.copy()\n",
    "\n",
    "    # Remplacer les '?' par NaN\n",
    "    df.replace('?', np.nan, inplace=True)\n",
    "\n",
    "    # Gestion des valeurs manquantes\n",
    "\n",
    "    ## Supprimer les colonnes avec trop de valeurs manquantes\n",
    "    # D'après l'EDA, nous pouvons supprimer 'number of colored vessels by fluoroscopy' et 'thalassemia'\n",
    "    df = df.drop(columns=['number of colored vessels by fluoroscopy', 'thalassemia'])\n",
    "\n",
    "    ## Imputer les valeurs manquantes pour les colonnes numériques\n",
    "    numerical_columns = [\n",
    "        'age', 'resting blood pressure (mm Hg)', 'cholestoral (mg/dl)',\n",
    "        'max heart rate', 'oldpeak'\n",
    "    ]\n",
    "    for col in numerical_columns:\n",
    "        df[col] = df[col].astype(float)\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "    ## Imputer les valeurs manquantes pour les colonnes catégorielles\n",
    "    categorical_columns = [\n",
    "        'sex', 'chest pain', 'fasting blood sugar > 120 mg/dl',\n",
    "        'resting electrocardiograph', 'exercise induced angina', 'slope'\n",
    "    ]\n",
    "    for col in categorical_columns:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "    # Encodage des variables catégorielles\n",
    "\n",
    "    ## Encodage binaire\n",
    "    df['sex'] = df['sex'].map({'male': 1, 'female': 0})\n",
    "    df['fasting blood sugar > 120 mg/dl'] = df['fasting blood sugar > 120 mg/dl'].map({'yes': 1, 'no': 0})\n",
    "    df['exercise induced angina'] = df['exercise induced angina'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "    ## Encodage one-hot pour les colonnes avec plus de deux catégories\n",
    "    df = pd.get_dummies(df, columns=['chest pain', 'resting electrocardiograph', 'slope'], drop_first=True)\n",
    "\n",
    "    # S'assurer que la colonne cible est présente et non modifiée\n",
    "    if 'class' not in df.columns:\n",
    "        raise KeyError(\"'class' column not found in DataFrame\")\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      age  sex  resting blood pressure (mm Hg)  cholestoral (mg/dl)  \\\n",
      "109  51.0    0                           150.0                200.0   \n",
      "270  55.0    1                           110.0                277.0   \n",
      "194  49.0    1                           140.0                228.0   \n",
      "357  32.0    1                           125.0                254.0   \n",
      "164  58.0    1                           105.0                240.0   \n",
      "\n",
      "     fasting blood sugar > 120 mg/dl  max heart rate  exercise induced angina  \\\n",
      "109                                0           120.0                        0   \n",
      "270                                0           160.0                        0   \n",
      "194                                0           130.0                        0   \n",
      "357                                0           155.0                        0   \n",
      "164                                0           154.0                        1   \n",
      "\n",
      "     oldpeak       class  chest pain_atypical angina  \\\n",
      "109      0.5  no disease                       False   \n",
      "270      0.0  no disease                       False   \n",
      "194      0.0  no disease                       False   \n",
      "357      0.0  no disease                        True   \n",
      "164      0.6  no disease                       False   \n",
      "\n",
      "     chest pain_non-anginal pain  chest pain_typical angina  \\\n",
      "109                         True                      False   \n",
      "270                         True                      False   \n",
      "194                        False                      False   \n",
      "357                        False                      False   \n",
      "164                         True                      False   \n",
      "\n",
      "     resting electrocardiograph_hypertrophy  \\\n",
      "109                                   False   \n",
      "270                                   False   \n",
      "194                                   False   \n",
      "357                                   False   \n",
      "164                                    True   \n",
      "\n",
      "     resting electrocardiograph_normal  slope_flat  slope_upsloping  \n",
      "109                               True       False             True  \n",
      "270                               True        True            False  \n",
      "194                               True        True            False  \n",
      "357                               True        True            False  \n",
      "164                              False        True            False  \n",
      "      age  sex  resting blood pressure (mm Hg)  cholestoral (mg/dl)  \\\n",
      "14   39.0    0                            94.0                199.0   \n",
      "425  56.0    1                           150.0                230.0   \n",
      "444  35.0    0                           140.0                167.0   \n",
      "265  66.0    1                           120.0                302.0   \n",
      "249  58.0    1                           132.0                458.0   \n",
      "\n",
      "     fasting blood sugar > 120 mg/dl  max heart rate  exercise induced angina  \\\n",
      "14                                 0           179.0                        0   \n",
      "425                                0           124.0                        1   \n",
      "444                                0           150.0                        0   \n",
      "265                                0           151.0                        0   \n",
      "249                                1            69.0                        0   \n",
      "\n",
      "     oldpeak       class  chest pain_atypical angina  \\\n",
      "14       0.0  no disease                       False   \n",
      "425      1.5     disease                       False   \n",
      "444      0.0  no disease                       False   \n",
      "265      0.4  no disease                       False   \n",
      "249      1.0  no disease                       False   \n",
      "\n",
      "     chest pain_non-anginal pain  chest pain_typical angina  \\\n",
      "14                          True                      False   \n",
      "425                        False                      False   \n",
      "444                        False                      False   \n",
      "265                        False                      False   \n",
      "249                        False                      False   \n",
      "\n",
      "     resting electrocardiograph_hypertrophy  \\\n",
      "14                                    False   \n",
      "425                                   False   \n",
      "444                                   False   \n",
      "265                                    True   \n",
      "249                                   False   \n",
      "\n",
      "     resting electrocardiograph_normal  slope_flat  slope_upsloping  \n",
      "14                                True       False             True  \n",
      "425                              False        True            False  \n",
      "444                               True       False             True  \n",
      "265                              False        True            False  \n",
      "249                               True       False            False  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3911969470.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3911969470.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3911969470.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3911969470.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3911969470.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3911969470.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3911969470.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3911969470.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3911969470.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3911969470.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Séparer les données en df_train et df_test avec stratification\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, stratify=df['class'], random_state=42)\n",
    "\n",
    "# Préparer les données avec prepare_data_v2\n",
    "df_train_v2 = prepare_data_v2(df_train)\n",
    "df_test_v2 = prepare_data_v2(df_test)\n",
    "\n",
    "# Afficher un aperçu des données préparées\n",
    "print(df_train_v2.head())\n",
    "print(df_test_v2.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ModelName Data                      Hyperparameters  RandomSeed  \\\n",
      "0  LogisticRegression   v2  {'penalty': None, 'max_iter': 1000}  3362672484   \n",
      "\n",
      "   Score_f2  Score_precision  Score_recall  Score_accuracy  \\\n",
      "0  0.365854              0.6      0.333333        0.826087   \n",
      "\n",
      "                                  TrainedEstimator  \n",
      "0  LogisticRegression(max_iter=1000, penalty=None)  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Importer le modèle de régression logistique\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Entraîner et évaluer le modèle sur les données préparées avec v2\n",
    "results_v2 = evaluate_model(\n",
    "    df_train_v2,\n",
    "    df_test_v2,\n",
    "    'v2',\n",
    "    'LogisticRegression',\n",
    "    LogisticRegression,\n",
    "    {'penalty': None, 'max_iter': 1000}\n",
    ")\n",
    "\n",
    "# Afficher les résultats\n",
    "print(results_v2)\n",
    "\n",
    "# Enregistrer les résultats\n",
    "append_results(results_v2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Entraîner et évaluer le modèle\n",
    "results_dt = evaluate_model(\n",
    "    df_train_v2,\n",
    "    df_test_v2,\n",
    "    'v2',\n",
    "    'DecisionTreeClassifier',\n",
    "    DecisionTreeClassifier,\n",
    "    {'max_depth': None, 'random_state': 42}\n",
    ")\n",
    "\n",
    "append_results(results_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "results_rf = evaluate_model(\n",
    "    df_train_v2,\n",
    "    df_test_v2,\n",
    "    'v2',\n",
    "    'RandomForestClassifier',\n",
    "    RandomForestClassifier,\n",
    "    {'n_estimators': 100, 'random_state': 42}\n",
    ")\n",
    "\n",
    "append_results(results_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lr_l2 = evaluate_model(\n",
    "    df_train_v2,\n",
    "    df_test_v2,\n",
    "    'v2',\n",
    "    'LogisticRegression_L2',\n",
    "    LogisticRegression,\n",
    "    {'penalty': 'l2', 'C': 1.0, 'solver': 'liblinear', 'max_iter': 1000}\n",
    ")\n",
    "\n",
    "append_results(results_lr_l2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essayer différentes valeurs pour max_depth\n",
    "for depth in [3, 5, 7, None]:\n",
    "    results_dt = evaluate_model(\n",
    "        df_train_v2,\n",
    "        df_test_v2,\n",
    "        'v2',\n",
    "        f'DecisionTreeClassifier_max_depth_{depth}',\n",
    "        DecisionTreeClassifier,\n",
    "        {'max_depth': depth, 'random_state': 42}\n",
    "    )\n",
    "    append_results(results_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_v3(df):\n",
    "    # Importer les bibliothèques nécessaires\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    # Copier le DataFrame pour éviter de le modifier directement\n",
    "    df = df.copy()\n",
    "\n",
    "    # Remplacer les '?' par NaN\n",
    "    df.replace('?', np.nan, inplace=True)\n",
    "\n",
    "    # Gestion des valeurs manquantes\n",
    "\n",
    "    # Supprimer les colonnes avec trop de valeurs manquantes\n",
    "    df = df.drop(columns=['number of colored vessels by fluoroscopy', 'thalassemia'])\n",
    "\n",
    "    # Imputer les valeurs manquantes pour les colonnes numériques\n",
    "    numerical_columns = [\n",
    "        'age', 'resting blood pressure (mm Hg)', 'cholestoral (mg/dl)',\n",
    "        'max heart rate', 'oldpeak'\n",
    "    ]\n",
    "    for col in numerical_columns:\n",
    "        df[col] = df[col].astype(float)\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "    # Imputer les valeurs manquantes pour les colonnes catégorielles\n",
    "    categorical_columns = [\n",
    "        'sex', 'chest pain', 'fasting blood sugar > 120 mg/dl',\n",
    "        'resting electrocardiograph', 'exercise induced angina', 'slope'\n",
    "    ]\n",
    "    for col in categorical_columns:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "    # Encodage des variables catégorielles\n",
    "\n",
    "    # Encodage binaire\n",
    "    df['sex'] = df['sex'].map({'male': 1, 'female': 0})\n",
    "    df['fasting blood sugar > 120 mg/dl'] = df['fasting blood sugar > 120 mg/dl'].map({'yes': 1, 'no': 0})\n",
    "    df['exercise induced angina'] = df['exercise induced angina'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "    # Encodage one-hot pour les colonnes avec plus de deux catégories\n",
    "    df = pd.get_dummies(df, columns=['chest pain', 'resting electrocardiograph', 'slope'], drop_first=True)\n",
    "\n",
    "    # Normalisation des variables numériques\n",
    "    scaler = StandardScaler()\n",
    "    df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "    # S'assurer que la colonne cible est présente et non modifiée\n",
    "    if 'class' not in df.columns:\n",
    "        raise KeyError(\"'class' column not found in DataFrame\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3968901949.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3968901949.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3968901949.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3968901949.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3968901949.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3968901949.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3968901949.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3968901949.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3968901949.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "/var/folders/g9/jzqz957j1slb_dkhyp9fl5_w0000gn/T/ipykernel_66927/3968901949.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          age  sex  resting blood pressure (mm Hg)  cholestoral (mg/dl)  \\\n",
      "109 -0.097300    0                        1.030776            -0.270998   \n",
      "270  0.322807    1                       -1.209643             0.652558   \n",
      "194 -0.307354    1                        0.470671             0.064841   \n",
      "357 -2.092809    1                       -0.369486             0.376691   \n",
      "164  0.637887    1                       -1.489695             0.208772   \n",
      "\n",
      "     fasting blood sugar > 120 mg/dl  max heart rate  exercise induced angina  \\\n",
      "109                                0       -0.972118                        0   \n",
      "270                                0        0.663398                        0   \n",
      "194                                0       -0.563239                        0   \n",
      "357                                0        0.458958                        0   \n",
      "164                                0        0.418070                        1   \n",
      "\n",
      "      oldpeak       class  chest pain_atypical angina  \\\n",
      "109 -0.043592  no disease                       False   \n",
      "270 -0.631762  no disease                       False   \n",
      "194 -0.631762  no disease                       False   \n",
      "357 -0.631762  no disease                        True   \n",
      "164  0.074042  no disease                       False   \n",
      "\n",
      "     chest pain_non-anginal pain  chest pain_typical angina  \\\n",
      "109                         True                      False   \n",
      "270                         True                      False   \n",
      "194                        False                      False   \n",
      "357                        False                      False   \n",
      "164                         True                      False   \n",
      "\n",
      "     resting electrocardiograph_hypertrophy  \\\n",
      "109                                   False   \n",
      "270                                   False   \n",
      "194                                   False   \n",
      "357                                   False   \n",
      "164                                    True   \n",
      "\n",
      "     resting electrocardiograph_normal  slope_flat  slope_upsloping  \n",
      "109                               True       False             True  \n",
      "270                               True        True            False  \n",
      "194                               True        True            False  \n",
      "357                               True        True            False  \n",
      "164                              False        True            False  \n",
      "          age  sex  resting blood pressure (mm Hg)  cholestoral (mg/dl)  \\\n",
      "14  -1.391153    0                       -2.305608            -0.138403   \n",
      "425  0.489368    1                        1.231763             0.194699   \n",
      "444 -1.833628    0                        0.600090            -0.482251   \n",
      "265  1.595557    1                       -0.663257             0.968356   \n",
      "249  0.710606    1                        0.094751             2.644613   \n",
      "\n",
      "     fasting blood sugar > 120 mg/dl  max heart rate  exercise induced angina  \\\n",
      "14                                 0        1.255730                        0   \n",
      "425                                0       -0.867222                        1   \n",
      "444                                0        0.136356                        0   \n",
      "265                                0        0.174955                        0   \n",
      "249                                1       -2.990174                        0   \n",
      "\n",
      "      oldpeak       class  chest pain_atypical angina  \\\n",
      "14  -0.642066  no disease                       False   \n",
      "425  0.971870     disease                       False   \n",
      "444 -0.642066  no disease                       False   \n",
      "265 -0.211683  no disease                       False   \n",
      "249  0.433891  no disease                       False   \n",
      "\n",
      "     chest pain_non-anginal pain  chest pain_typical angina  \\\n",
      "14                          True                      False   \n",
      "425                        False                      False   \n",
      "444                        False                      False   \n",
      "265                        False                      False   \n",
      "249                        False                      False   \n",
      "\n",
      "     resting electrocardiograph_hypertrophy  \\\n",
      "14                                    False   \n",
      "425                                   False   \n",
      "444                                   False   \n",
      "265                                    True   \n",
      "249                                   False   \n",
      "\n",
      "     resting electrocardiograph_normal  slope_flat  slope_upsloping  \n",
      "14                                True       False             True  \n",
      "425                              False        True            False  \n",
      "444                               True       False             True  \n",
      "265                              False        True            False  \n",
      "249                               True       False            False  \n"
     ]
    }
   ],
   "source": [
    "# Préparer les données avec prepare_data_v3\n",
    "df_train_v3 = prepare_data_v3(df_train)\n",
    "df_test_v3 = prepare_data_v3(df_test)\n",
    "\n",
    "# Afficher un aperçu des données préparées\n",
    "print(df_train_v3.head())\n",
    "print(df_test_v3.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          ModelName Data  \\\n",
      "0  LogisticRegression_L2_Normalized   v3   \n",
      "\n",
      "                                     Hyperparameters  RandomSeed  Score_f2  \\\n",
      "0  {'penalty': 'l2', 'C': 1.0, 'solver': 'libline...   190479754  0.301205   \n",
      "\n",
      "   Score_precision  Score_recall  Score_accuracy  \\\n",
      "0         0.454545      0.277778        0.793478   \n",
      "\n",
      "                                    TrainedEstimator  \n",
      "0  LogisticRegression(max_iter=1000, solver='libl...  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Entraîner et évaluer le modèle\n",
    "results_lr_v3 = evaluate_model(\n",
    "    df_train_v3,\n",
    "    df_test_v3,\n",
    "    'v3',\n",
    "    'LogisticRegression_L2_Normalized',\n",
    "    LogisticRegression,\n",
    "    {'penalty': 'l2', 'C': 1.0, 'solver': 'liblinear', 'max_iter': 1000}\n",
    ")\n",
    "\n",
    "# Enregistrer les résultats\n",
    "append_results(results_lr_v3)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(results_lr_v3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       ModelName Data  \\\n",
      "0  DecisionTreeClassifier_max_depth_5_Normalized   v3   \n",
      "\n",
      "                        Hyperparameters  RandomSeed  Score_f2  \\\n",
      "0  {'max_depth': 5, 'random_state': 42}  3786572474  0.483871   \n",
      "\n",
      "   Score_precision  Score_recall  Score_accuracy  \\\n",
      "0         0.428571           0.5        0.771739   \n",
      "\n",
      "                                    TrainedEstimator  \n",
      "0  DecisionTreeClassifier(max_depth=5, random_sta...  \n"
     ]
    }
   ],
   "source": [
    "# Entraîner et évaluer le modèle avec max_depth=5 (meilleur jusqu'à présent)\n",
    "results_dt_v3 = evaluate_model(\n",
    "    df_train_v3,\n",
    "    df_test_v3,\n",
    "    'v3',\n",
    "    'DecisionTreeClassifier_max_depth_5_Normalized',\n",
    "    DecisionTreeClassifier,\n",
    "    {'max_depth': 5, 'random_state': 42}\n",
    ")\n",
    "\n",
    "# Enregistrer les résultats\n",
    "append_results(results_dt_v3)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(results_dt_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dt_v3_optimized = evaluate_model(\n",
    "    df_train_v3,\n",
    "    df_test_v3,\n",
    "    'v3',\n",
    "    'DecisionTreeClassifier_Optimized',\n",
    "    DecisionTreeClassifier,\n",
    "    {\n",
    "        'max_depth': 5,\n",
    "        'min_samples_split': 5,\n",
    "        'min_samples_leaf': 2,\n",
    "        'criterion': 'entropy',\n",
    "        'random_state': 42\n",
    "    }\n",
    ")\n",
    "\n",
    "append_results(results_dt_v3_optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_v4(df):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.experimental import enable_iterative_imputer  # Nécessaire pour utiliser IterativeImputer\n",
    "    from sklearn.impute import IterativeImputer\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    # Copier le DataFrame pour éviter de le modifier directement\n",
    "    df = df.copy()\n",
    "\n",
    "    # Remplacer les '?' par NaN\n",
    "    df.replace('?', np.nan, inplace=True)\n",
    "\n",
    "    # Supprimer les colonnes avec trop de valeurs manquantes\n",
    "    # On peut tenter d'imputer 'thalassemia' si on pense que c'est une variable importante\n",
    "    df = df.drop(columns=['number of colored vessels by fluoroscopy'])\n",
    "\n",
    "    # Encodage des variables catégorielles\n",
    "\n",
    "    # Encodage binaire\n",
    "    df['sex'] = df['sex'].map({'male': 1, 'female': 0})\n",
    "    df['fasting blood sugar > 120 mg/dl'] = df['fasting blood sugar > 120 mg/dl'].map({'yes': 1, 'no': 0})\n",
    "    df['exercise induced angina'] = df['exercise induced angina'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "    # Gérer les valeurs manquantes dans les colonnes catégorielles\n",
    "    categorical_columns = ['sex', 'fasting blood sugar > 120 mg/dl', 'exercise induced angina',\n",
    "                           'chest pain', 'resting electrocardiograph', 'slope', 'thalassemia']\n",
    "    for col in categorical_columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "    # Encodage one-hot pour les variables catégorielles restantes\n",
    "    df = pd.get_dummies(df, columns=['chest pain', 'resting electrocardiograph', 'slope', 'thalassemia'], drop_first=True)\n",
    "\n",
    "    # Séparer la cible\n",
    "    y = df['class']\n",
    "    X = df.drop(columns=['class'])\n",
    "\n",
    "    # Imputation multiple avec IterativeImputer\n",
    "    imputer = IterativeImputer(random_state=42)\n",
    "    X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "    # Normalisation des variables numériques\n",
    "    numerical_columns = [\n",
    "        'age', 'resting blood pressure (mm Hg)', 'cholestoral (mg/dl)',\n",
    "        'max heart rate', 'oldpeak'\n",
    "    ]\n",
    "    scaler = StandardScaler()\n",
    "    X_imputed[numerical_columns] = scaler.fit_transform(X_imputed[numerical_columns])\n",
    "\n",
    "    # Reconstituer le DataFrame\n",
    "    df_prepared = pd.concat([X_imputed, y.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    return df_prepared\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_data(X, y):\n",
    "    from sklearn.utils import resample\n",
    "    \n",
    "    # Combiner X et y\n",
    "    df = pd.concat([X, y], axis=1)\n",
    "    \n",
    "    # Séparer les classes majoritaire et minoritaire\n",
    "    df_majority = df[df['class'] == 'no disease']\n",
    "    df_minority = df[df['class'] == 'disease']\n",
    "    \n",
    "    # Downsample de la classe majoritaire\n",
    "    df_majority_downsampled = resample(\n",
    "        df_majority,\n",
    "        replace=False,\n",
    "        n_samples=len(df_minority),\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Combiner les deux classes\n",
    "    df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "    \n",
    "    # Séparer X et y\n",
    "    X_downsampled = df_downsampled.drop(columns=['class'])\n",
    "    y_downsampled = df_downsampled['class']\n",
    "    \n",
    "    return X_downsampled, y_downsampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparer les données en df_train et df_test avec stratification\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, stratify=df['class'], random_state=42)\n",
    "\n",
    "# Préparer les données avec prepare_data_v4\n",
    "df_train_v4 = prepare_data_v4(df_train)\n",
    "df_test_v4 = prepare_data_v4(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparer X et y\n",
    "X_train_v4 = df_train_v4.drop(columns=['class'])\n",
    "y_train_v4 = df_train_v4['class']\n",
    "X_test_v4 = df_test_v4.drop(columns=['class'])\n",
    "y_test_v4 = df_test_v4['class']\n",
    "\n",
    "# Appliquer le downsampling\n",
    "X_train_downsampled, y_train_downsampled = downsample_data(X_train_v4, y_train_v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            ModelName            Data  \\\n",
      "0  DecisionTreeClassifier_Downsampled  v4_downsampled   \n",
      "\n",
      "                        Hyperparameters  RandomSeed  Score_f2  \\\n",
      "0  {'max_depth': 5, 'random_state': 42}  3177593907  0.490196   \n",
      "\n",
      "   Score_precision  Score_recall  Score_accuracy  \\\n",
      "0         0.333333      0.555556        0.695652   \n",
      "\n",
      "                                    TrainedEstimator  \n",
      "0  DecisionTreeClassifier(max_depth=5, random_sta...  \n"
     ]
    }
   ],
   "source": [
    "# Recréer df_train_downsampled\n",
    "df_train_downsampled = pd.concat([X_train_downsampled, y_train_downsampled], axis=1)\n",
    "\n",
    "# Entraîner et évaluer le modèle\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "results_downsampled = evaluate_model(\n",
    "    df_train_downsampled,\n",
    "    df_test_v4,\n",
    "    'v4_downsampled',\n",
    "    'DecisionTreeClassifier_Downsampled',\n",
    "    DecisionTreeClassifier,\n",
    "    {'max_depth': 5, 'random_state': 42}\n",
    ")\n",
    "\n",
    "# Enregistrer les résultats\n",
    "append_results(results_downsampled)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(results_downsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_data(X, y):\n",
    "    from sklearn.utils import resample\n",
    "    \n",
    "    # Combiner X et y\n",
    "    df = pd.concat([X, y], axis=1)\n",
    "    \n",
    "    # Séparer les classes majoritaire et minoritaire\n",
    "    df_majority = df[df['class'] == 'no disease']\n",
    "    df_minority = df[df['class'] == 'disease']\n",
    "    \n",
    "    # Upsample de la classe minoritaire\n",
    "    df_minority_upsampled = resample(\n",
    "        df_minority,\n",
    "        replace=True,\n",
    "        n_samples=len(df_majority),\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Combiner les deux classes\n",
    "    df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "    \n",
    "    # Séparer X et y\n",
    "    X_upsampled = df_upsampled.drop(columns=['class'])\n",
    "    y_upsampled = df_upsampled['class']\n",
    "    \n",
    "    return X_upsampled, y_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer l'upsampling\n",
    "X_train_upsampled, y_train_upsampled = upsample_data(X_train_v4, y_train_v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          ModelName          Data  \\\n",
      "0  DecisionTreeClassifier_Upsampled  v4_upsampled   \n",
      "\n",
      "                        Hyperparameters  RandomSeed  Score_f2  \\\n",
      "0  {'max_depth': 5, 'random_state': 42}  2583452187  0.436893   \n",
      "\n",
      "   Score_precision  Score_recall  Score_accuracy  \\\n",
      "0         0.290323           0.5        0.663043   \n",
      "\n",
      "                                    TrainedEstimator  \n",
      "0  DecisionTreeClassifier(max_depth=5, random_sta...  \n"
     ]
    }
   ],
   "source": [
    "# Recréer df_train_upsampled\n",
    "df_train_upsampled = pd.concat([X_train_upsampled, y_train_upsampled], axis=1)\n",
    "\n",
    "# Entraîner et évaluer le modèle\n",
    "results_upsampled = evaluate_model(\n",
    "    df_train_upsampled,\n",
    "    df_test_v4,\n",
    "    'v4_upsampled',\n",
    "    'DecisionTreeClassifier_Upsampled',\n",
    "    DecisionTreeClassifier,\n",
    "    {'max_depth': 5, 'random_state': 42}\n",
    ")\n",
    "\n",
    "# Enregistrer les résultats\n",
    "append_results(results_upsampled)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(results_upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         ModelName         Data  \\\n",
      "0  DecisionTreeClassifier_Weighted  v4_weighted   \n",
      "\n",
      "                                     Hyperparameters  RandomSeed  Score_f2  \\\n",
      "0  {'max_depth': 5, 'random_state': 42, 'class_we...  2093047478  0.360825   \n",
      "\n",
      "   Score_precision  Score_recall  Score_accuracy  \\\n",
      "0             0.28      0.388889        0.684783   \n",
      "\n",
      "                                    TrainedEstimator  \n",
      "0  DecisionTreeClassifier(class_weight={'disease'...  \n"
     ]
    }
   ],
   "source": [
    "# Définir les poids de classe\n",
    "class_weights = {'no disease': 1, 'disease': 3}  # Vous pouvez ajuster les poids selon les besoins\n",
    "\n",
    "# Entraîner et évaluer le modèle\n",
    "results_weighted = evaluate_model(\n",
    "    df_train_v4,\n",
    "    df_test_v4,\n",
    "    'v4_weighted',\n",
    "    'DecisionTreeClassifier_Weighted',\n",
    "    DecisionTreeClassifier,\n",
    "    {'max_depth': 5, 'random_state': 42, 'class_weight': class_weights}\n",
    ")\n",
    "\n",
    "# Enregistrer les résultats\n",
    "append_results(results_weighted)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(results_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def smote_data(X, y):\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_smote, y_smote = smote.fit_resample(X, y)\n",
    "    return X_smote, y_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer SMOTE\n",
    "X_train_smote, y_train_smote = smote_data(X_train_v4, y_train_v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      ModelName      Data  \\\n",
      "0  DecisionTreeClassifier_SMOTE  v4_smote   \n",
      "\n",
      "                        Hyperparameters  RandomSeed  Score_f2  \\\n",
      "0  {'max_depth': 5, 'random_state': 42}  3058381753  0.376344   \n",
      "\n",
      "   Score_precision  Score_recall  Score_accuracy  \\\n",
      "0         0.333333      0.388889        0.728261   \n",
      "\n",
      "                                    TrainedEstimator  \n",
      "0  DecisionTreeClassifier(max_depth=5, random_sta...  \n"
     ]
    }
   ],
   "source": [
    "# Recréer df_train_smote\n",
    "df_train_smote = pd.concat([X_train_smote, y_train_smote], axis=1)\n",
    "\n",
    "# Entraîner et évaluer le modèle\n",
    "results_smote = evaluate_model(\n",
    "    df_train_smote,\n",
    "    df_test_v4,\n",
    "    'v4_smote',\n",
    "    'DecisionTreeClassifier_SMOTE',\n",
    "    DecisionTreeClassifier,\n",
    "    {'max_depth': 5, 'random_state': 42}\n",
    ")\n",
    "\n",
    "# Enregistrer les résultats\n",
    "append_results(results_smote)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(results_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_hyperparameters(model_class, param_grid, X_train, y_train):\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.metrics import make_scorer, fbeta_score\n",
    "\n",
    "    # Créer un scorer personnalisé pour le F-beta score avec beta=2\n",
    "    f2_scorer = make_scorer(fbeta_score, beta=2, pos_label='disease')\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model_class(),\n",
    "        param_grid=param_grid,\n",
    "        scoring=f2_scorer,\n",
    "        refit=True,\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search.best_estimator_, grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs paramètres pour LogisticRegression avec Downsampling : {'C': 0.1, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "                        ModelName            Data  \\\n",
      "0  LogisticRegression_Downsampled  v4_downsampled   \n",
      "\n",
      "                                     Hyperparameters  RandomSeed  Score_f2  \\\n",
      "0  {'C': 0.1, 'max_iter': 1000, 'penalty': 'l2', ...   381573423       0.6   \n",
      "\n",
      "   Score_precision  Score_recall  Score_accuracy  \\\n",
      "0         0.428571      0.666667         0.76087   \n",
      "\n",
      "                                    TrainedEstimator  \n",
      "0  LogisticRegression(C=0.1, max_iter=1000, solve...  \n"
     ]
    }
   ],
   "source": [
    "# Définir la grille de paramètres pour LogisticRegression\n",
    "param_grid_lr = {\n",
    "    'penalty': ['l2'],  # 'liblinear' supporte 'l1' et 'l2'\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['liblinear'],\n",
    "    'max_iter': [1000]\n",
    "}\n",
    "\n",
    "# Optimiser les hyperparamètres\n",
    "best_estimator_lr_down, best_params_lr_down = optimize_hyperparameters(\n",
    "    LogisticRegression,\n",
    "    param_grid_lr,\n",
    "    X_train_downsampled,\n",
    "    y_train_downsampled\n",
    ")\n",
    "\n",
    "print(\"Meilleurs paramètres pour LogisticRegression avec Downsampling :\", best_params_lr_down)\n",
    "\n",
    "# Évaluer le modèle optimisé\n",
    "results_lr_down = evaluate_model(\n",
    "    df_train_downsampled,\n",
    "    df_test_v4,\n",
    "    'v4_downsampled',\n",
    "    'LogisticRegression_Downsampled',\n",
    "    LogisticRegression,\n",
    "    best_params_lr_down\n",
    ")\n",
    "\n",
    "# Enregistrer les résultats\n",
    "append_results(results_lr_down)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(results_lr_down)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs paramètres pour LogisticRegression avec Upsampling : {'C': 10, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "                      ModelName          Data  \\\n",
      "0  LogisticRegression_Upsampled  v4_upsampled   \n",
      "\n",
      "                                     Hyperparameters  RandomSeed  Score_f2  \\\n",
      "0  {'C': 10, 'max_iter': 1000, 'penalty': 'l2', '...  3412526607  0.549451   \n",
      "\n",
      "   Score_precision  Score_recall  Score_accuracy  \\\n",
      "0         0.526316      0.555556        0.815217   \n",
      "\n",
      "                                    TrainedEstimator  \n",
      "0  LogisticRegression(C=10, max_iter=1000, solver...  \n"
     ]
    }
   ],
   "source": [
    "# Optimiser les hyperparamètres\n",
    "best_estimator_lr_up, best_params_lr_up = optimize_hyperparameters(\n",
    "    LogisticRegression,\n",
    "    param_grid_lr,\n",
    "    X_train_upsampled,\n",
    "    y_train_upsampled\n",
    ")\n",
    "\n",
    "print(\"Meilleurs paramètres pour LogisticRegression avec Upsampling :\", best_params_lr_up)\n",
    "\n",
    "# Évaluer le modèle optimisé\n",
    "results_lr_up = evaluate_model(\n",
    "    df_train_upsampled,\n",
    "    df_test_v4,\n",
    "    'v4_upsampled',\n",
    "    'LogisticRegression_Upsampled',\n",
    "    LogisticRegression,\n",
    "    best_params_lr_up\n",
    ")\n",
    "\n",
    "# Enregistrer les résultats\n",
    "append_results(results_lr_up)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(results_lr_up)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs paramètres pour LogisticRegression avec Loss Weighting : {'C': 0.1, 'class_weight': {'no disease': 1, 'disease': 3}, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "                     ModelName         Data  \\\n",
      "0  LogisticRegression_Weighted  v4_weighted   \n",
      "\n",
      "                                     Hyperparameters  RandomSeed  Score_f2  \\\n",
      "0  {'C': 0.1, 'class_weight': {'no disease': 1, '...   590938642  0.531915   \n",
      "\n",
      "   Score_precision  Score_recall  Score_accuracy  \\\n",
      "0         0.454545      0.555556        0.782609   \n",
      "\n",
      "                                    TrainedEstimator  \n",
      "0  LogisticRegression(C=0.1, class_weight={'disea...  \n"
     ]
    }
   ],
   "source": [
    "# Définir les poids de classe\n",
    "class_weights_lr = {'no disease': 1, 'disease': 3}\n",
    "\n",
    "# Définir la grille de paramètres avec class_weight\n",
    "param_grid_lr_weighted = {\n",
    "    'penalty': ['l2'],\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['liblinear'],\n",
    "    'max_iter': [1000],\n",
    "    'class_weight': [class_weights_lr]\n",
    "}\n",
    "\n",
    "# Optimiser les hyperparamètres\n",
    "best_estimator_lr_weighted, best_params_lr_weighted = optimize_hyperparameters(\n",
    "    LogisticRegression,\n",
    "    param_grid_lr_weighted,\n",
    "    X_train_v4,\n",
    "    y_train_v4\n",
    ")\n",
    "\n",
    "print(\"Meilleurs paramètres pour LogisticRegression avec Loss Weighting :\", best_params_lr_weighted)\n",
    "\n",
    "# Évaluer le modèle optimisé\n",
    "df_train_weighted = pd.concat([X_train_v4, y_train_v4], axis=1)\n",
    "results_lr_weighted = evaluate_model(\n",
    "    df_train_weighted,\n",
    "    df_test_v4,\n",
    "    'v4_weighted',\n",
    "    'LogisticRegression_Weighted',\n",
    "    LogisticRegression,\n",
    "    best_params_lr_weighted\n",
    ")\n",
    "\n",
    "# Enregistrer les résultats\n",
    "append_results(results_lr_weighted)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(results_lr_weighted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs paramètres pour LogisticRegression avec SMOTE : {'C': 1, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "                  ModelName      Data  \\\n",
      "0  LogisticRegression_SMOTE  v4_smote   \n",
      "\n",
      "                                     Hyperparameters  RandomSeed  Score_f2  \\\n",
      "0  {'C': 1, 'max_iter': 1000, 'penalty': 'l2', 's...  3337360932   0.48913   \n",
      "\n",
      "   Score_precision  Score_recall  Score_accuracy  \\\n",
      "0             0.45           0.5        0.782609   \n",
      "\n",
      "                                    TrainedEstimator  \n",
      "0  LogisticRegression(C=1, max_iter=1000, solver=...  \n"
     ]
    }
   ],
   "source": [
    "# Optimiser les hyperparamètres\n",
    "best_estimator_lr_smote, best_params_lr_smote = optimize_hyperparameters(\n",
    "    LogisticRegression,\n",
    "    param_grid_lr,\n",
    "    X_train_smote,\n",
    "    y_train_smote\n",
    ")\n",
    "\n",
    "print(\"Meilleurs paramètres pour LogisticRegression avec SMOTE :\", best_params_lr_smote)\n",
    "\n",
    "# Évaluer le modèle optimisé\n",
    "results_lr_smote = evaluate_model(\n",
    "    df_train_smote,\n",
    "    df_test_v4,\n",
    "    'v4_smote',\n",
    "    'LogisticRegression_SMOTE',\n",
    "    LogisticRegression,\n",
    "    best_params_lr_smote\n",
    ")\n",
    "\n",
    "# Enregistrer les résultats\n",
    "append_results(results_lr_smote)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(results_lr_smote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs paramètres pour RandomForestClassifier avec Downsampling : {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100, 'random_state': 42}\n",
      "                            ModelName            Data  \\\n",
      "0  RandomForestClassifier_Downsampled  v4_downsampled   \n",
      "\n",
      "                                     Hyperparameters  RandomSeed  Score_f2  \\\n",
      "0  {'max_depth': None, 'max_features': 'sqrt', 'm...  1603038928  0.576923   \n",
      "\n",
      "   Score_precision  Score_recall  Score_accuracy  \\\n",
      "0            0.375      0.666667        0.717391   \n",
      "\n",
      "                                    TrainedEstimator  \n",
      "0  (DecisionTreeClassifier(max_features='sqrt', r...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "120 fits failed out of a total of 240.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "92 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "28 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.82508619 0.81698773 0.81482243 0.80053655\n",
      " 0.81515857 0.81751128 0.80026552 0.82407504        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.81266383 0.81698773 0.80399992 0.80053655 0.81515857 0.81751128\n",
      " 0.80026552 0.82407504        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.82508619 0.81698773\n",
      " 0.81482243 0.80053655 0.81515857 0.81751128 0.80026552 0.82407504]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Définir la grille de paramètres pour RandomForestClassifier\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "# Optimiser les hyperparamètres\n",
    "best_estimator_rf_down, best_params_rf_down = optimize_hyperparameters(\n",
    "    RandomForestClassifier,\n",
    "    param_grid_rf,\n",
    "    X_train_downsampled,\n",
    "    y_train_downsampled\n",
    ")\n",
    "\n",
    "print(\"Meilleurs paramètres pour RandomForestClassifier avec Downsampling :\", best_params_rf_down)\n",
    "\n",
    "# Évaluer le modèle optimisé\n",
    "results_rf_down = evaluate_model(\n",
    "    df_train_downsampled,\n",
    "    df_test_v4,\n",
    "    'v4_downsampled',\n",
    "    'RandomForestClassifier_Downsampled',\n",
    "    RandomForestClassifier,\n",
    "    best_params_rf_down\n",
    ")\n",
    "\n",
    "# Enregistrer les résultats\n",
    "append_results(results_rf_down)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(results_rf_down)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs paramètres pour RandomForestClassifier avec Downsampling : {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100, 'random_state': 42}\n",
      "                            ModelName            Data  \\\n",
      "0  RandomForestClassifier_Downsampled  v4_downsampled   \n",
      "\n",
      "                                     Hyperparameters  RandomSeed  Score_f2  \\\n",
      "0  {'max_depth': None, 'max_features': 'sqrt', 'm...  1102679409  0.576923   \n",
      "\n",
      "   Score_precision  Score_recall  Score_accuracy  \\\n",
      "0            0.375      0.666667        0.717391   \n",
      "\n",
      "                                    TrainedEstimator  \n",
      "0  (DecisionTreeClassifier(max_features='sqrt', r...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "120 fits failed out of a total of 240.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "95 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.82508619 0.81698773 0.81482243 0.80053655\n",
      " 0.81515857 0.81751128 0.80026552 0.82407504        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.81266383 0.81698773 0.80399992 0.80053655 0.81515857 0.81751128\n",
      " 0.80026552 0.82407504        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.82508619 0.81698773\n",
      " 0.81482243 0.80053655 0.81515857 0.81751128 0.80026552 0.82407504]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Définir la grille de paramètres pour RandomForestClassifier\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "# Optimiser les hyperparamètres\n",
    "best_estimator_rf_down, best_params_rf_down = optimize_hyperparameters(\n",
    "    RandomForestClassifier,\n",
    "    param_grid_rf,\n",
    "    X_train_downsampled,\n",
    "    y_train_downsampled\n",
    ")\n",
    "\n",
    "print(\"Meilleurs paramètres pour RandomForestClassifier avec Downsampling :\", best_params_rf_down)\n",
    "\n",
    "# Évaluer le modèle optimisé\n",
    "results_rf_down = evaluate_model(\n",
    "    df_train_downsampled,\n",
    "    df_test_v4,\n",
    "    'v4_downsampled',\n",
    "    'RandomForestClassifier_Downsampled',\n",
    "    RandomForestClassifier,\n",
    "    best_params_rf_down\n",
    ")\n",
    "\n",
    "# Enregistrer les résultats\n",
    "append_results(results_rf_down)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(results_rf_down)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs paramètres pour RandomForestClassifier avec Upsampling : {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100, 'random_state': 42}\n",
      "                          ModelName          Data  \\\n",
      "0  RandomForestClassifier_Upsampled  v4_upsampled   \n",
      "\n",
      "                                     Hyperparameters  RandomSeed  Score_f2  \\\n",
      "0  {'max_depth': 10, 'max_features': 'sqrt', 'min...  3830736073  0.666667   \n",
      "\n",
      "   Score_precision  Score_recall  Score_accuracy  \\\n",
      "0         0.666667      0.666667        0.869565   \n",
      "\n",
      "                                    TrainedEstimator  \n",
      "0  (DecisionTreeClassifier(max_depth=10, max_feat...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "120 fits failed out of a total of 240.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "32 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "88 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.97289281 0.97277591 0.95666776 0.95534891\n",
      " 0.96008047 0.95736368 0.95947493 0.95275572        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.97353334 0.96545831 0.95538063 0.95393647 0.95673938 0.95800771\n",
      " 0.95882399 0.95882399        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.97289281 0.97277591\n",
      " 0.95666776 0.95534891 0.96008047 0.95736368 0.95947493 0.95275572]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Appliquer Upsampling\n",
    "X_train_upsampled, y_train_upsampled = upsample_data(X_train_v4, y_train_v4)\n",
    "df_train_upsampled = pd.concat([X_train_upsampled, y_train_upsampled], axis=1)\n",
    "\n",
    "# Optimiser les hyperparamètres\n",
    "best_estimator_rf_up, best_params_rf_up = optimize_hyperparameters(\n",
    "    RandomForestClassifier,\n",
    "    param_grid_rf,\n",
    "    X_train_upsampled,\n",
    "    y_train_upsampled\n",
    ")\n",
    "\n",
    "print(\"Meilleurs paramètres pour RandomForestClassifier avec Upsampling :\", best_params_rf_up)\n",
    "\n",
    "# Évaluer le modèle optimisé\n",
    "results_rf_up = evaluate_model(\n",
    "    df_train_upsampled,\n",
    "    df_test_v4,\n",
    "    'v4_upsampled',\n",
    "    'RandomForestClassifier_Upsampled',\n",
    "    RandomForestClassifier,\n",
    "    best_params_rf_up\n",
    ")\n",
    "\n",
    "# Enregistrer les résultats\n",
    "append_results(results_rf_up)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(results_rf_up)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs paramètres pour RandomForestClassifier avec Loss Weighting : {'class_weight': {'no disease': 1, 'disease': 3}, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100, 'random_state': 42}\n",
      "                         ModelName         Data  \\\n",
      "0  RandomForestClassifier_Weighted  v4_weighted   \n",
      "\n",
      "                                     Hyperparameters  RandomSeed  Score_f2  \\\n",
      "0  {'class_weight': {'no disease': 1, 'disease': ...   803945210  0.465116   \n",
      "\n",
      "   Score_precision  Score_recall  Score_accuracy  \\\n",
      "0         0.571429      0.444444        0.826087   \n",
      "\n",
      "                                    TrainedEstimator  \n",
      "0  (DecisionTreeClassifier(max_features='sqrt', m...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "120 fits failed out of a total of 240.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "68 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "52 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.43515968 0.462001   0.51132192 0.5116145\n",
      " 0.52763282 0.50204007 0.53952321 0.52991145        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.45008484 0.46253525 0.51137049 0.50896648 0.5277729  0.50356214\n",
      " 0.53952321 0.53906511        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.43515968 0.462001\n",
      " 0.51132192 0.5116145  0.52763282 0.50204007 0.53952321 0.52991145]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Définir les poids de classe pour RandomForestClassifier\n",
    "class_weights_rf = {'no disease': 1, 'disease': 3}\n",
    "\n",
    "# Définir la grille de paramètres avec class_weight\n",
    "param_grid_rf_weighted = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'random_state': [42],\n",
    "    'class_weight': [class_weights_rf]\n",
    "}\n",
    "\n",
    "# Optimiser les hyperparamètres\n",
    "best_estimator_rf_weighted, best_params_rf_weighted = optimize_hyperparameters(\n",
    "    RandomForestClassifier,\n",
    "    param_grid_rf_weighted,\n",
    "    X_train_v4,\n",
    "    y_train_v4\n",
    ")\n",
    "\n",
    "print(\"Meilleurs paramètres pour RandomForestClassifier avec Loss Weighting :\", best_params_rf_weighted)\n",
    "\n",
    "# Évaluer le modèle optimisé\n",
    "df_train_weighted_rf = pd.concat([X_train_v4, y_train_v4], axis=1)\n",
    "results_rf_weighted = evaluate_model(\n",
    "    df_train_weighted_rf,\n",
    "    df_test_v4,\n",
    "    'v4_weighted',\n",
    "    'RandomForestClassifier_Weighted',\n",
    "    RandomForestClassifier,\n",
    "    best_params_rf_weighted\n",
    ")\n",
    "\n",
    "# Enregistrer les résultats\n",
    "append_results(results_rf_weighted)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(results_rf_weighted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs paramètres pour RandomForestClassifier avec SMOTE : {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100, 'random_state': 42}\n",
      "                      ModelName      Data  \\\n",
      "0  RandomForestClassifier_SMOTE  v4_smote   \n",
      "\n",
      "                                     Hyperparameters  RandomSeed  Score_f2  \\\n",
      "0  {'max_depth': None, 'max_features': 'sqrt', 'm...  1149985292       0.5   \n",
      "\n",
      "   Score_precision  Score_recall  Score_accuracy  \\\n",
      "0              0.5           0.5        0.804348   \n",
      "\n",
      "                                    TrainedEstimator  \n",
      "0  (DecisionTreeClassifier(max_features='sqrt', r...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "120 fits failed out of a total of 240.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "87 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "33 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.90650066 0.9006369  0.89184813 0.89515355\n",
      " 0.88965835 0.89166752 0.89316817 0.8944853         nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.90355873 0.90383001 0.89244154 0.8919733  0.89338482 0.88521871\n",
      " 0.88825298 0.893843          nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.90650066 0.9006369\n",
      " 0.89184813 0.89515355 0.88965835 0.89166752 0.89316817 0.8944853 ]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Optimiser les hyperparamètres\n",
    "best_estimator_rf_smote, best_params_rf_smote = optimize_hyperparameters(\n",
    "    RandomForestClassifier,\n",
    "    param_grid_rf,\n",
    "    X_train_smote,\n",
    "    y_train_smote\n",
    ")\n",
    "\n",
    "print(\"Meilleurs paramètres pour RandomForestClassifier avec SMOTE :\", best_params_rf_smote)\n",
    "\n",
    "# Évaluer le modèle optimisé\n",
    "results_rf_smote = evaluate_model(\n",
    "    df_train_smote,\n",
    "    df_test_v4,\n",
    "    'v4_smote',\n",
    "    'RandomForestClassifier_SMOTE',\n",
    "    RandomForestClassifier,\n",
    "    best_params_rf_smote\n",
    ")\n",
    "\n",
    "# Enregistrer les résultats\n",
    "append_results(results_rf_smote)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(results_rf_smote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "param_grid_bagging = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_samples': [0.5, 1.0],\n",
    "    'max_features': [0.5, 1.0],\n",
    "    'bootstrap': [True, False],\n",
    "    'random_state': [42]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, fbeta_score\n",
    "\n",
    "def optimize_hyperparameters(model_class, param_grid, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Optimise les hyperparamètres d'un modèle en utilisant GridSearchCV avec un scorer personnalisé F2.\n",
    "    \n",
    "    Parameters:\n",
    "        model_class: La classe du modèle (e.g., DecisionTreeClassifier).\n",
    "        param_grid (dict): Grille des hyperparamètres à tester.\n",
    "        X_train (DataFrame): Variables explicatives d'entraînement.\n",
    "        y_train (Series): Variable cible d'entraînement.\n",
    "    \n",
    "    Returns:\n",
    "        best_estimator_: Le meilleur modèle trouvé.\n",
    "        best_params_: Les meilleurs hyperparamètres trouvés.\n",
    "    \"\"\"\n",
    "    # Créer un scorer personnalisé pour le F2-score\n",
    "    f2_scorer = make_scorer(fbeta_score, beta=2, pos_label='disease')\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model_class(),\n",
    "        param_grid=param_grid,\n",
    "        scoring=f2_scorer,\n",
    "        refit=True,\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search.best_estimator_, grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def smote_data(X, y):\n",
    "    \"\"\"\n",
    "    Applique SMOTE pour générer des données synthétiques de la classe minoritaire.\n",
    "    \n",
    "    Parameters:\n",
    "        X (DataFrame): Variables explicatives.\n",
    "        y (Series): Variable cible.\n",
    "    \n",
    "    Returns:\n",
    "        X_smote (DataFrame): Variables explicatives SMOTE.\n",
    "        y_smote (Series): Variable cible SMOTE.\n",
    "    \"\"\"\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_smote, y_smote = smote.fit_resample(X, y)\n",
    "    return X_smote, y_smote\n",
    "\n",
    "# Appliquer SMOTE\n",
    "X_train_smote_bagging, y_train_smote_bagging = smote_data(X_train_v4, y_train_v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs paramètres pour BaggingClassifier avec SMOTE : {'bootstrap': True, 'max_features': 1.0, 'max_samples': 0.5, 'n_estimators': 50, 'random_state': 42}\n",
      "                 ModelName              Data  \\\n",
      "0  BaggingClassifier_SMOTE  v4_bagging_smote   \n",
      "\n",
      "                                     Hyperparameters  RandomSeed  Score_f2  \\\n",
      "0  {'bootstrap': True, 'max_features': 1.0, 'max_...  1795035241  0.483871   \n",
      "\n",
      "   Score_precision  Score_recall  Score_accuracy  \\\n",
      "0         0.428571           0.5        0.771739   \n",
      "\n",
      "                                    TrainedEstimator  \n",
      "0  (DecisionTreeClassifier(random_state=195292617...  \n"
     ]
    }
   ],
   "source": [
    "# Optimiser les hyperparamètres pour BaggingClassifier\n",
    "best_estimator_bagging, best_params_bagging = optimize_hyperparameters(\n",
    "    BaggingClassifier,\n",
    "    param_grid_bagging,\n",
    "    X_train_smote_bagging,\n",
    "    y_train_smote_bagging\n",
    ")\n",
    "\n",
    "print(\"Meilleurs paramètres pour BaggingClassifier avec SMOTE :\", best_params_bagging)\n",
    "\n",
    "# Recréer df_train_bagging_smote\n",
    "df_train_bagging_smote = pd.concat([X_train_smote_bagging, y_train_smote_bagging], axis=1)\n",
    "\n",
    "# Entraîner et évaluer le modèle\n",
    "results_bagging = evaluate_model(\n",
    "    df_train_bagging_smote,\n",
    "    df_test_v4,\n",
    "    'v4_bagging_smote',\n",
    "    'BaggingClassifier_SMOTE',\n",
    "    BaggingClassifier,\n",
    "    best_params_bagging\n",
    ")\n",
    "\n",
    "# Enregistrer les résultats\n",
    "append_results(results_bagging)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(results_bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "param_grid_boosting = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'random_state': [42]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs paramètres pour GradientBoostingClassifier avec SMOTE : {'learning_rate': 0.2, 'max_depth': 7, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 42}\n",
      "                          ModelName               Data  \\\n",
      "0  GradientBoostingClassifier_SMOTE  v4_boosting_smote   \n",
      "\n",
      "                                     Hyperparameters  RandomSeed  Score_f2  \\\n",
      "0  {'learning_rate': 0.2, 'max_depth': 7, 'min_sa...  4176449941  0.494505   \n",
      "\n",
      "   Score_precision  Score_recall  Score_accuracy  \\\n",
      "0         0.473684           0.5        0.793478   \n",
      "\n",
      "                                    TrainedEstimator  \n",
      "0  ([DecisionTreeRegressor(criterion='friedman_ms...  \n"
     ]
    }
   ],
   "source": [
    "# Optimiser les hyperparamètres pour GradientBoostingClassifier\n",
    "best_estimator_boosting, best_params_boosting = optimize_hyperparameters(\n",
    "    GradientBoostingClassifier,\n",
    "    param_grid_boosting,\n",
    "    X_train_smote,\n",
    "    y_train_smote\n",
    ")\n",
    "\n",
    "print(\"Meilleurs paramètres pour GradientBoostingClassifier avec SMOTE :\", best_params_boosting)\n",
    "\n",
    "# Recréer df_train_boosting_smote\n",
    "df_train_boosting_smote = pd.concat([X_train_smote, y_train_smote], axis=1)\n",
    "\n",
    "# Entraîner et évaluer le modèle\n",
    "results_boosting = evaluate_model(\n",
    "    df_train_boosting_smote,\n",
    "    df_test_v4,\n",
    "    'v4_boosting_smote',\n",
    "    'GradientBoostingClassifier_SMOTE',\n",
    "    GradientBoostingClassifier,\n",
    "    best_params_boosting\n",
    ")\n",
    "\n",
    "# Enregistrer les résultats\n",
    "append_results(results_boosting)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(results_boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N'oubliez pas de mettre à jour vos fichiers `models_results.csv` et `predictions.csv` !!!\n",
    "\n",
    "- `models_results.csv` à chaque expérimentation effectuée ;\n",
    "- et `predictions.csv` une fois votre meilleur modèle identifié.\n",
    "\n",
    "Ces fichiers sont à rendre sur Moodle avec votre fichier `tp4.ipynb`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
